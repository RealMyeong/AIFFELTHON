{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd40f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from soynlp.word import WordExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0e074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/aiffel/aiffel/AIFFELTHON/data/train_RI100_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb22b2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게', '만', '게임', '겜', '되', '음', '면']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55006215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 0.544 Gbry 0.409 Gb\n",
      "all cohesion probabilities was computed. # words = 1877\n",
      "all branching entropies was computed # words = 52472\n",
      "all accessor variety was computed # words = 52472\n"
     ]
    }
   ],
   "source": [
    "sentences = data.non_label_sentence\n",
    "sentences = [sen for sen in sentences]\n",
    "\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "word_extractor = WordExtractor(\n",
    "    min_frequency=100, # example\n",
    "    min_cohesion_forward=0.05,\n",
    "    min_right_branching_entropy=0.0\n",
    ")\n",
    "\n",
    "word_extractor.train(sentences)\n",
    "words = word_extractor.extract()\n",
    "\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in words.items()}\n",
    "tokenizer = LTokenizer(scores=cohesion_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a8d8083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_sentence</th>\n",
       "      <th>non_label_sentence</th>\n",
       "      <th>class</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1:안녕하세요 친절 상담원 입니다\\n2:저 수강중인게 있는데 일시정지를 하고 싶어서...</td>\n",
       "      <td>친절\\n저 수강중인게 있는데 일시정지를 하고 싶어서요\\n일시정지는 강좌 당 일회 가...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>[친절, 저, 수강중인게, 있는데, 일시정지를, 하고, 싶어, 서요, 일시정지는, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1:욕실 발 매트 새로 사고 싶다\\n2:왜 망가졌나\\n1:지금 쓰는 규조토 물을 이...</td>\n",
       "      <td>욕실 발 매트 새로 사고 싶다\\n왜 망가졌나\\n지금 쓰는 규조토 물을 이제 더 이상...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>[욕실, 발, 매트, 새로, 사고, 싶다, 왜, 망가졌나, 지금, 쓰는, 규조토, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1:안녕하세요 입니다 민원인께서 문의 주셨습니다\\n1:아산 음봉에 율지천 하천 점용...</td>\n",
       "      <td>민원인께서 문의 주셨습니다\\n아산 음봉에 율지천 하천 점용 관련으로 하천안전과 연결...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>[민원인께서, 문의, 주셨습니다, 아산, 음봉에, 율지천, 하천, 점용, 관련, 으...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1:반려동물을 씻기는거 말고도 고난이도가 뭐가 있어\\n2:발톱 깎이는거야\\n1:그런...</td>\n",
       "      <td>반려동물을 씻기는거 말고도 고난이도가 뭐가 있어\\n발톱 깎이는거야\\n그런데 왜 힘들...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>[반려동물을, 씻기는거, 말고, 고난이도가, 뭐가, 있어, 발톱, 깎이는거야, 그런...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1:데이트 몰아서 하는거 아닌가여\\n2:일주일에 두 번만 보면 돼\\n2:더 보면 싸...</td>\n",
       "      <td>데이트 몰아서 하는거 아닌가여\\n일주일에 두  번만 보면 돼\\n더 보면 싸운다\\n노...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>[데이, 트, 몰아서, 하는, 거, 아닌가, 여, 일주일, 두, 번만, 보면, 돼,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>1:준하야 넌 대가리가 왜 이렇게 크냐\\n2:내 머리가 뭐\\n1:밥 먹으면 대가리만...</td>\n",
       "      <td>준 우승자 하야 스즈키 넌 플리트 대가리 가 왜 과연 이렇게 크냐\\n내 내고 머리 ...</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>공격 대화</td>\n",
       "      <td>[준, 우승자, 하야, 스즈키, 넌, 플리트, 대가리, 왜, 과연, 이렇게, 크냐,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>1:내가 지금 너 아들 김길준 데리고 있어  살리고 싶으면 계좌에 1억만 보내\\n2...</td>\n",
       "      <td>내 내고 가 지금 현재 너 에드 아들 딸 김길준 데리 거느리 고 있어 살리고 싶으면...</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>공격 대화</td>\n",
       "      <td>[내, 내고, 지금, 현재, 너, 에드, 아들, 딸, 김길준, 데리, 거느리, 있어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>1:나는 씨 같은 사람 보면 참 신기하더라  어떻게 저렇게 살지\\n1:왜 그래  들...</td>\n",
       "      <td>나 나의 는 씨 정씨 같은 사람 젊은이 보면 참 신기하더라 어떻게 저렇게 살 살아오...</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>공격 대화</td>\n",
       "      <td>[나, 나의, 씨, 정씨, 같은, 사람, 젊은, 보면, 참, 신기, 하더라, 어떻게...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>1:누구 맘대로 여기서 장사하래\\n2:이게 무슨 일입니까\\n1:남의 구역에서 장사하...</td>\n",
       "      <td>누구 어디 맘대로 여기 알리 서 장사 태수 하래\\n이 그러 게 무슨 어찌 일 입 입...</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>공격 대화</td>\n",
       "      <td>[누구, 어디, 맘대로, 여기, 알리, 서, 장사, 태수, 하래, 그러, 무슨, 어...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>1:희정 씨\\n2:네\\n1:주말에 시간이 넘쳐나나 봐\\n2:갑자기 왜 그러세요\\n1...</td>\n",
       "      <td>희 정 정이 씨 정씨\\n네 카나\\n주말 토요일 에 시간 분간 이 넘쳐나나 봐\\n갑자...</td>\n",
       "      <td>직장 내 괴롭힘 대화</td>\n",
       "      <td>공격 대화</td>\n",
       "      <td>[희, 정, 정이, 씨, 정씨, 카나, 주말, 토요일, 시간, 분간, 넘쳐나나, 봐...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9740 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         label_sentence  \\\n",
       "0     1:안녕하세요 친절 상담원 입니다\\n2:저 수강중인게 있는데 일시정지를 하고 싶어서...   \n",
       "1     1:욕실 발 매트 새로 사고 싶다\\n2:왜 망가졌나\\n1:지금 쓰는 규조토 물을 이...   \n",
       "2     1:안녕하세요 입니다 민원인께서 문의 주셨습니다\\n1:아산 음봉에 율지천 하천 점용...   \n",
       "3     1:반려동물을 씻기는거 말고도 고난이도가 뭐가 있어\\n2:발톱 깎이는거야\\n1:그런...   \n",
       "4     1:데이트 몰아서 하는거 아닌가여\\n2:일주일에 두 번만 보면 돼\\n2:더 보면 싸...   \n",
       "...                                                 ...   \n",
       "9735  1:준하야 넌 대가리가 왜 이렇게 크냐\\n2:내 머리가 뭐\\n1:밥 먹으면 대가리만...   \n",
       "9736  1:내가 지금 너 아들 김길준 데리고 있어  살리고 싶으면 계좌에 1억만 보내\\n2...   \n",
       "9737  1:나는 씨 같은 사람 보면 참 신기하더라  어떻게 저렇게 살지\\n1:왜 그래  들...   \n",
       "9738  1:누구 맘대로 여기서 장사하래\\n2:이게 무슨 일입니까\\n1:남의 구역에서 장사하...   \n",
       "9739  1:희정 씨\\n2:네\\n1:주말에 시간이 넘쳐나나 봐\\n2:갑자기 왜 그러세요\\n1...   \n",
       "\n",
       "                                     non_label_sentence        class  \\\n",
       "0     친절\\n저 수강중인게 있는데 일시정지를 하고 싶어서요\\n일시정지는 강좌 당 일회 가...        일반 대화   \n",
       "1     욕실 발 매트 새로 사고 싶다\\n왜 망가졌나\\n지금 쓰는 규조토 물을 이제 더 이상...        일반 대화   \n",
       "2     민원인께서 문의 주셨습니다\\n아산 음봉에 율지천 하천 점용 관련으로 하천안전과 연결...        일반 대화   \n",
       "3     반려동물을 씻기는거 말고도 고난이도가 뭐가 있어\\n발톱 깎이는거야\\n그런데 왜 힘들...        일반 대화   \n",
       "4     데이트 몰아서 하는거 아닌가여\\n일주일에 두  번만 보면 돼\\n더 보면 싸운다\\n노...        일반 대화   \n",
       "...                                                 ...          ...   \n",
       "9735  준 우승자 하야 스즈키 넌 플리트 대가리 가 왜 과연 이렇게 크냐\\n내 내고 머리 ...    기타 괴롭힘 대화   \n",
       "9736  내 내고 가 지금 현재 너 에드 아들 딸 김길준 데리 거느리 고 있어 살리고 싶으면...        갈취 대화   \n",
       "9737  나 나의 는 씨 정씨 같은 사람 젊은이 보면 참 신기하더라 어떻게 저렇게 살 살아오...  직장 내 괴롭힘 대화   \n",
       "9738  누구 어디 맘대로 여기 알리 서 장사 태수 하래\\n이 그러 게 무슨 어찌 일 입 입...        갈취 대화   \n",
       "9739  희 정 정이 씨 정씨\\n네 카나\\n주말 토요일 에 시간 분간 이 넘쳐나나 봐\\n갑자...  직장 내 괴롭힘 대화   \n",
       "\n",
       "     binary_class                                          tokenized  \n",
       "0           일반 대화  [친절, 저, 수강중인게, 있는데, 일시정지를, 하고, 싶어, 서요, 일시정지는, ...  \n",
       "1           일반 대화  [욕실, 발, 매트, 새로, 사고, 싶다, 왜, 망가졌나, 지금, 쓰는, 규조토, ...  \n",
       "2           일반 대화  [민원인께서, 문의, 주셨습니다, 아산, 음봉에, 율지천, 하천, 점용, 관련, 으...  \n",
       "3           일반 대화  [반려동물을, 씻기는거, 말고, 고난이도가, 뭐가, 있어, 발톱, 깎이는거야, 그런...  \n",
       "4           일반 대화  [데이, 트, 몰아서, 하는, 거, 아닌가, 여, 일주일, 두, 번만, 보면, 돼,...  \n",
       "...           ...                                                ...  \n",
       "9735        공격 대화  [준, 우승자, 하야, 스즈키, 넌, 플리트, 대가리, 왜, 과연, 이렇게, 크냐,...  \n",
       "9736        공격 대화  [내, 내고, 지금, 현재, 너, 에드, 아들, 딸, 김길준, 데리, 거느리, 있어...  \n",
       "9737        공격 대화  [나, 나의, 씨, 정씨, 같은, 사람, 젊은, 보면, 참, 신기, 하더라, 어떻게...  \n",
       "9738        공격 대화  [누구, 어디, 맘대로, 여기, 알리, 서, 장사, 태수, 하래, 그러, 무슨, 어...  \n",
       "9739        공격 대화  [희, 정, 정이, 씨, 정씨, 카나, 주말, 토요일, 시간, 분간, 넘쳐나나, 봐...  \n",
       "\n",
       "[9740 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized'] = data['non_label_sentence'].apply(tokenizer.tokenize)\n",
    "data['tokenized'] = data['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce560521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63184"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = data['tokenized'].values\n",
    "\n",
    "tokenizer_tf = Tokenizer()\n",
    "tokenizer_tf.fit_on_texts(conversation)\n",
    "len(tokenizer_tf.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21897693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 63184\n",
      "등장 빈도가 1번 이하인 희귀 단어의 수: 34243\n",
      "전체 단어 집합에서 희귀 단어를 뺀 크기 :  28941\n",
      "단어 집합에서 희귀 단어의 비율: 54.19568245125348\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.032103280624639\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "total_cnt = len(tokenizer_tf.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer_tf.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('전체 단어 집합에서 희귀 단어를 뺀 크기 : ', total_cnt - rare_cnt)\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfb36a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000#total_cnt - rare_cnt\n",
    "tokenizer_tf = Tokenizer(vocab_size) \n",
    "tokenizer_tf.fit_on_texts(conversation)\n",
    "X_data = tokenizer_tf.texts_to_sequences(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15830d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...  1758  1086   328]\n",
      " [    0     0     0 ...  2039     4  7205]\n",
      " [    0     0     0 ...  1042  1414 28973]\n",
      " ...\n",
      " [    0     0     0 ...     7     2   263]\n",
      " [    0     0     0 ...  4042  6015   803]\n",
      " [    0     0     0 ...  7186  1719    15]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(X_data)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "518ef1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = padded\n",
    "train_label = data['class']\n",
    "\n",
    "labels = {'협박 대화':0, '갈취 대화':1, '직장 내 괴롭힘 대화':2, '기타 괴롭힘 대화':3, '일반 대화':4}\n",
    "\n",
    "train_label = train_label.apply(lambda x : labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17d0d7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7792 974 974\n",
      "7792 974 974\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_data, train_label, test_size=0.2, random_state=22, stratify=train_label)\n",
    "valid_X, test_X, valid_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=22, stratify=test_Y)\n",
    "\n",
    "print(len(train_X), len(valid_X), len(test_X))\n",
    "print(len(train_Y), len(valid_Y), len(test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2833514c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4057    2\n",
       "3794    0\n",
       "4004    0\n",
       "4715    3\n",
       "5678    3\n",
       "       ..\n",
       "8801    3\n",
       "7180    1\n",
       "8874    1\n",
       "9099    3\n",
       "7062    3\n",
       "Name: class, Length: 7792, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bae3d68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97c2a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e10dd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = train_X.shape[1]\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(len(labels), activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85dcb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1b64cdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "244/244 [==============================] - 4s 11ms/step - loss: 1.3683 - accuracy: 0.4026 - val_loss: 0.6157 - val_accuracy: 0.8439\n",
      "Epoch 2/10\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.3670 - accuracy: 0.8818 - val_loss: 0.2669 - val_accuracy: 0.9066\n",
      "Epoch 3/10\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.1268 - accuracy: 0.9660 - val_loss: 0.1882 - val_accuracy: 0.9374\n",
      "Epoch 4/10\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.0452 - accuracy: 0.9883 - val_loss: 0.2309 - val_accuracy: 0.9405\n",
      "Epoch 5/10\n",
      "244/244 [==============================] - 2s 10ms/step - loss: 0.0199 - accuracy: 0.9958 - val_loss: 0.1936 - val_accuracy: 0.9528\n",
      "Epoch 00005: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    train_X, train_Y, batch_size=32, epochs=10, validation_data=(valid_X, valid_Y), callbacks=[es]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "61a8a778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_003</th>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_495</th>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_496</th>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_497</th>\n",
       "      <td>야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_498</th>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_499</th>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "t_000  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...\n",
       "t_001  우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...\n",
       "t_002  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...\n",
       "t_003  이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...\n",
       "t_004  아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...\n",
       "...                                                  ...\n",
       "t_495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...\n",
       "t_496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...\n",
       "t_497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...\n",
       "t_498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...\n",
       "t_499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....\n",
       "\n",
       "[500 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_path = '/aiffel/aiffel/AIFFELTHON/data/test.json'\n",
    "with open(test_file_path, mode='rt', encoding='utf-8') as f:\n",
    "    test_dataset = pd.read_json(f)\n",
    "\n",
    "test_data = test_dataset.transpose()\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "45c6d3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_003</th>\n",
       "      <td>이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "t_000  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...\n",
       "t_001  우리팀에서 다른팀으로 갈 사람 없나 그럼 영지씨가 가는건 어때 네 제가요 그렇지 2...\n",
       "t_002  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요 제대로 좀 하지 네 똑바로 좀 하지 행실...\n",
       "t_003  이거 들어바 와 이 노래 진짜 좋다 그치 요즘 이 것만 들어 진짜 너무 좋다 내가 ...\n",
       "t_004  아무튼 앞으로 니가 내 와이파이야 응 와이파이 온 켰어 반말 주인님이라고도 말해야지..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_punctuation(x):\n",
    "  x = x.strip()\n",
    "  x = re.sub(\"[^가-힣0-9]+\", \" \", x)\n",
    "  x = re.sub(\"[ ]+\", \" \", x)\n",
    "  x = x.strip()\n",
    "  return x\n",
    "\n",
    "test_data[\"text\"] = test_data['text'].apply(lambda x : remove_punctuation(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbe9d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sen for sen in test_data['text']]\n",
    "sentences = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "sequences = tokenizer_tf.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbbc7435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 578,  692,   54, ...,    0,    0,    0],\n",
       "       [  37, 6860,  128, ...,    0,    0,    0],\n",
       "       [   3,   57,  100, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    3, 1433, ...,    0,    0,    0],\n",
       "       [   1,    3,   78, ...,    0,    0,    0],\n",
       "       [  85,   22,   47, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57f9b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(padded)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred_label = []\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    pred_label.append((str)(np.argmax(pred[i])).zfill(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b8c2164f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['03',\n",
       " '02',\n",
       " '02',\n",
       " '04',\n",
       " '04',\n",
       " '00',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '00',\n",
       " '04',\n",
       " '02',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '01',\n",
       " '00',\n",
       " '03',\n",
       " '02',\n",
       " '01',\n",
       " '03',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '04',\n",
       " '00',\n",
       " '01',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '04',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '01',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '00',\n",
       " '04',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '02',\n",
       " '00',\n",
       " '00',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '04',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '04',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '00',\n",
       " '04',\n",
       " '04',\n",
       " '03',\n",
       " '04',\n",
       " '00',\n",
       " '03',\n",
       " '04',\n",
       " '00',\n",
       " '03',\n",
       " '00',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '01',\n",
       " '02',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '04',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '02',\n",
       " '00',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '00',\n",
       " '02',\n",
       " '01',\n",
       " '01',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '00',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '00',\n",
       " '00',\n",
       " '03',\n",
       " '01',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '00',\n",
       " '02',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '00',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '02',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '02',\n",
       " '00',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '02',\n",
       " '00',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '01',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '04',\n",
       " '03',\n",
       " '00',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '00',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '00',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '04',\n",
       " '04',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '01',\n",
       " '03',\n",
       " '02',\n",
       " '00',\n",
       " '02',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '02',\n",
       " '03',\n",
       " '02',\n",
       " '04',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '00',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '03',\n",
       " '03',\n",
       " '04',\n",
       " '00',\n",
       " '01',\n",
       " '03',\n",
       " '02',\n",
       " '03',\n",
       " '03',\n",
       " '02',\n",
       " '03']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "efd6029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission = test_data.assign(CLASS=pred_label)\n",
    "submission = submission.rename(columns={'CLASS':'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "762545c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t_000</th>\n",
       "      <th>t_001</th>\n",
       "      <th>t_002</th>\n",
       "      <th>t_003</th>\n",
       "      <th>t_004</th>\n",
       "      <th>t_005</th>\n",
       "      <th>t_006</th>\n",
       "      <th>t_007</th>\n",
       "      <th>t_008</th>\n",
       "      <th>t_009</th>\n",
       "      <th>...</th>\n",
       "      <th>t_490</th>\n",
       "      <th>t_491</th>\n",
       "      <th>t_492</th>\n",
       "      <th>t_493</th>\n",
       "      <th>t_494</th>\n",
       "      <th>t_495</th>\n",
       "      <th>t_496</th>\n",
       "      <th>t_497</th>\n",
       "      <th>t_498</th>\n",
       "      <th>t_499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>03</td>\n",
       "      <td>02</td>\n",
       "      <td>02</td>\n",
       "      <td>04</td>\n",
       "      <td>04</td>\n",
       "      <td>00</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>03</td>\n",
       "      <td>01</td>\n",
       "      <td>...</td>\n",
       "      <td>03</td>\n",
       "      <td>04</td>\n",
       "      <td>00</td>\n",
       "      <td>01</td>\n",
       "      <td>03</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "      <td>03</td>\n",
       "      <td>02</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      t_000 t_001 t_002 t_003 t_004 t_005 t_006 t_007 t_008 t_009  ... t_490  \\\n",
       "class    03    02    02    04    04    00    02    03    03    01  ...    03   \n",
       "\n",
       "      t_491 t_492 t_493 t_494 t_495 t_496 t_497 t_498 t_499  \n",
       "class    04    00    01    03    02    03    03    02    03  \n",
       "\n",
       "[1 rows x 500 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.drop(['text'], axis=1, inplace=True)\n",
    "submission = submission.transpose()\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7da64d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "submission_file_path = '/aiffel/aiffel/AIFFELTHON/submission/submission_transformer_RI100_2000.json'\n",
    "result = submission.to_json(submission_file_path)\n",
    "\n",
    "with open(submission_file_path) as f:\n",
    "    parsed = json.load(f)\n",
    "\n",
    "with open(submission_file_path, 'w') as f:\n",
    "    json.dump(parsed, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
