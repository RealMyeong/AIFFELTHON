{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68128f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntensorflow = 2.8.0\\nkeras = 2.6.0\\ntransformers = 4.11.3\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tensorflow = 2.8.0\n",
    "keras = 2.6.0\n",
    "transformers = 4.11.3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee5ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4442b1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 29 04:50:27 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   35C    P0    39W / 300W |      0MiB / 16384MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714b5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/wlsaud5474/aiffelthon/notebook', '/home/wlsaud5474/anaconda3/envs/jinmyeong/lib/python37.zip', '/home/wlsaud5474/anaconda3/envs/jinmyeong/lib/python3.7', '/home/wlsaud5474/anaconda3/envs/jinmyeong/lib/python3.7/lib-dynload', '', '/home/wlsaud5474/anaconda3/envs/jinmyeong/lib/python3.7/site-packages', '/home/wlsaud5474/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/IPython/extensions', '/home/wlsaud5474/.ipython', '/home/wlsaud5474/aiffelthon']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getenv('HOME')+'/aiffelthon')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10395b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert import modeling_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9401f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import re\n",
    "import pyarrow as pa\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f03dd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>non_label_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ê¸¸ë™ ì”¨ ì´ë²ˆì— ì´ê²ƒ ì¢€ ì²˜ë¦¬í•´ìš”\\nì´ê±° ì œê°€ í•œ ê²Œ ì•„ë‹Œë°ìš”\\níŒ€ì—ì„œ ë‚´ê°€ ë„¤ê°€ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ì•¼ ë„ˆ ì•ë‹ˆ ì™œ ê·¸ë ‡ê²Œ íŠ€ì–´ë‚˜ì™”ëƒ\\níƒœì–´ë‚  ë•Œë¶€í„° ê·¸ë¬ì–´ ë¬¼ì–´ë³´ì§€ ë§ˆ\\nì•„ ê·¸ëŸ¼ íƒœ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì›í›„ì•¼ í•™ì› êµì¬ ì‚¬ì•¼ ë˜ëŠ”ë° 8ë§Œ ì›ë§Œ ì£¼ë©´ ì•ˆ ë¼\\në¯¸ì•ˆ ë‚˜ ì˜¤ëŠ˜ì€ ì§„ì§œ ëˆì´ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ë„ˆ ì €ë²ˆì— ìˆ  ë¨¹ì€ ë‚  ê¸°ì–µí•´\\nì•„ë‹ˆ ì™œ\\nì•¼ 300ë§Œ ì›ë§Œ ê°€ì ¸ì™€\\në­” ì†Œë¦¬ì•¼  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ë„ˆ ë˜ ë‚´ë§ ë¬´ì‹œí•˜ëƒ\\nì´ ìƒˆë¼ ë„ ì¢†ìœ¼ë¡œ ë³´ë‚˜ ë³¸ë°\\nì•„ëƒ ì§„ì§œ ì‹œê°„ì´ ì—†ì—ˆì–´ ë¯¸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26970</th>\n",
       "      <td>ì¼ë°˜ ëŒ€í™”</td>\n",
       "      <td>í˜¹ì‹œ ìê¸°ì†Œê°œì„œ ì¨ë´¤ì–´\\ní•œë²ˆë„ ì•ˆ ì¨ë´¤ì–´\\nìê¸°ì†Œê°œì„œ ì“°ëŠ” ê²ƒ ì •ë§ í˜ë“¤ì–´\\në„ˆëŠ”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26971</th>\n",
       "      <td>ì¼ë°˜ ëŒ€í™”</td>\n",
       "      <td>ë‚˜ ì´ë²ˆì— í™”ìƒìœ¼ë¡œ ìˆ˜ì—… ë“£ë‹¤ê°€ ì¡¸ì•˜ì–ì•„ í‚¤í‚¤\\ní‚¤í‚¤ ì¸ê°• ì•ˆ ë“£ë‹¤ê°€ ì™ ì¼ë¡œ ë“¤ì—ˆì–´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26972</th>\n",
       "      <td>ì¼ë°˜ ëŒ€í™”</td>\n",
       "      <td>ë¹„í–‰ê¸° íƒ€ ë³¸ ì  ìˆì–´\\në‚˜ ê³ ë“±í•™êµ ì¡¸ì—…ì—¬í–‰ ë•Œ í•œ ë²ˆ\\nì˜¤ ì¡¸ì—…ì—¬í–‰ë„ ê°”ì—ˆë‚˜\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26973</th>\n",
       "      <td>ì¼ë°˜ ëŒ€í™”</td>\n",
       "      <td>ë„ˆëŠ” í•™ìƒ ë•Œ ì–´ë–¤ ìŒ¤ì´ ì œì¼ ì¢‹ì•˜ì–´\\në‚˜ëŠ”   ë³„ë¡œ ì¢‹ì•˜ë˜ ìŒ¤ì´ ì—†ì—ˆì–´\\nì¤‘í•™ìƒ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26974</th>\n",
       "      <td>ì¼ë°˜ ëŒ€í™”</td>\n",
       "      <td>ë„ì™€ ë“œë¦´ê¹Œìš”\\nê´€ë ¨í•´ì„œ ë¬¸ì˜ ì¢€ í• ê²Œìš”\\në„¤ ì–´ë–¤ ë¬¸ì˜ë¡œ ì—°ë½ì£¼ì…¨ë‚˜ìš”\\nì œê°€ ì œí’ˆ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26975 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             class                                 non_label_sentence\n",
       "0      ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  ê¸¸ë™ ì”¨ ì´ë²ˆì— ì´ê²ƒ ì¢€ ì²˜ë¦¬í•´ìš”\\nì´ê±° ì œê°€ í•œ ê²Œ ì•„ë‹Œë°ìš”\\níŒ€ì—ì„œ ë‚´ê°€ ë„¤ê°€ ...\n",
       "1        ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì•¼ ë„ˆ ì•ë‹ˆ ì™œ ê·¸ë ‡ê²Œ íŠ€ì–´ë‚˜ì™”ëƒ\\níƒœì–´ë‚  ë•Œë¶€í„° ê·¸ë¬ì–´ ë¬¼ì–´ë³´ì§€ ë§ˆ\\nì•„ ê·¸ëŸ¼ íƒœ...\n",
       "2            ê°ˆì·¨ ëŒ€í™”  ì›í›„ì•¼ í•™ì› êµì¬ ì‚¬ì•¼ ë˜ëŠ”ë° 8ë§Œ ì›ë§Œ ì£¼ë©´ ì•ˆ ë¼\\në¯¸ì•ˆ ë‚˜ ì˜¤ëŠ˜ì€ ì§„ì§œ ëˆì´ ...\n",
       "3            ê°ˆì·¨ ëŒ€í™”  ë„ˆ ì €ë²ˆì— ìˆ  ë¨¹ì€ ë‚  ê¸°ì–µí•´\\nì•„ë‹ˆ ì™œ\\nì•¼ 300ë§Œ ì›ë§Œ ê°€ì ¸ì™€\\në­” ì†Œë¦¬ì•¼  ...\n",
       "4            í˜‘ë°• ëŒ€í™”  ë„ˆ ë˜ ë‚´ë§ ë¬´ì‹œí•˜ëƒ\\nì´ ìƒˆë¼ ë„ ì¢†ìœ¼ë¡œ ë³´ë‚˜ ë³¸ë°\\nì•„ëƒ ì§„ì§œ ì‹œê°„ì´ ì—†ì—ˆì–´ ë¯¸...\n",
       "...            ...                                                ...\n",
       "26970        ì¼ë°˜ ëŒ€í™”  í˜¹ì‹œ ìê¸°ì†Œê°œì„œ ì¨ë´¤ì–´\\ní•œë²ˆë„ ì•ˆ ì¨ë´¤ì–´\\nìê¸°ì†Œê°œì„œ ì“°ëŠ” ê²ƒ ì •ë§ í˜ë“¤ì–´\\në„ˆëŠ”...\n",
       "26971        ì¼ë°˜ ëŒ€í™”  ë‚˜ ì´ë²ˆì— í™”ìƒìœ¼ë¡œ ìˆ˜ì—… ë“£ë‹¤ê°€ ì¡¸ì•˜ì–ì•„ í‚¤í‚¤\\ní‚¤í‚¤ ì¸ê°• ì•ˆ ë“£ë‹¤ê°€ ì™ ì¼ë¡œ ë“¤ì—ˆì–´...\n",
       "26972        ì¼ë°˜ ëŒ€í™”  ë¹„í–‰ê¸° íƒ€ ë³¸ ì  ìˆì–´\\në‚˜ ê³ ë“±í•™êµ ì¡¸ì—…ì—¬í–‰ ë•Œ í•œ ë²ˆ\\nì˜¤ ì¡¸ì—…ì—¬í–‰ë„ ê°”ì—ˆë‚˜\\n...\n",
       "26973        ì¼ë°˜ ëŒ€í™”  ë„ˆëŠ” í•™ìƒ ë•Œ ì–´ë–¤ ìŒ¤ì´ ì œì¼ ì¢‹ì•˜ì–´\\në‚˜ëŠ”   ë³„ë¡œ ì¢‹ì•˜ë˜ ìŒ¤ì´ ì—†ì—ˆì–´\\nì¤‘í•™ìƒ ...\n",
       "26974        ì¼ë°˜ ëŒ€í™”  ë„ì™€ ë“œë¦´ê¹Œìš”\\nê´€ë ¨í•´ì„œ ë¬¸ì˜ ì¢€ í• ê²Œìš”\\në„¤ ì–´ë–¤ ë¬¸ì˜ë¡œ ì—°ë½ì£¼ì…¨ë‚˜ìš”\\nì œê°€ ì œí’ˆ...\n",
       "\n",
       "[26975 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "data_dir = os.getenv('HOME')+'/aiffelthon/AIFFELTHON/TUNiB/experiment_data'\n",
    "data = pd.read_csv(f\"{data_dir}/train_bt_en_fr_ch_8000.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54642008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.drop(['label_sentence', 'binary_class'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3697c06c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>non_label_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ê¸¸ë™ ì”¨ ì´ë²ˆì— ì´ê²ƒ ì¢€ ì²˜ë¦¬í•´ìš”\\nì´ê±° ì œê°€ í•œ ê²Œ ì•„ë‹Œë°ìš”\\níŒ€ì—ì„œ ë‚´ê°€ ë„¤ê°€ ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ì•¼ ë„ˆ ì•ë‹ˆ ì™œ ê·¸ë ‡ê²Œ íŠ€ì–´ë‚˜ì™”ëƒ\\níƒœì–´ë‚  ë•Œë¶€í„° ê·¸ë¬ì–´ ë¬¼ì–´ë³´ì§€ ë§ˆ\\nì•„ ê·¸ëŸ¼ íƒœ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì›í›„ì•¼ í•™ì› êµì¬ ì‚¬ì•¼ ë˜ëŠ”ë° 8ë§Œ ì›ë§Œ ì£¼ë©´ ì•ˆ ë¼\\në¯¸ì•ˆ ë‚˜ ì˜¤ëŠ˜ì€ ì§„ì§œ ëˆì´ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ë„ˆ ì €ë²ˆì— ìˆ  ë¨¹ì€ ë‚  ê¸°ì–µí•´\\nì•„ë‹ˆ ì™œ\\nì•¼ 300ë§Œ ì›ë§Œ ê°€ì ¸ì™€\\në­” ì†Œë¦¬ì•¼  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>í˜‘ë°• ëŒ€í™”</td>\n",
       "      <td>ë„ˆ ë˜ ë‚´ë§ ë¬´ì‹œí•˜ëƒ\\nì´ ìƒˆë¼ ë„ ì¢†ìœ¼ë¡œ ë³´ë‚˜ ë³¸ë°\\nì•„ëƒ ì§„ì§œ ì‹œê°„ì´ ì—†ì—ˆì–´ ë¯¸...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì•¼ ë¼ì§€ ì˜¤ëŠ˜ë„ ëˆ ë‚´ë†”\\nì˜¤ëŠ˜ì€ ì§„ì§œ ì—†ì–´\\nì•„ìƒˆë¼ ì˜¤ëŠ˜ ì§„ì§œ ëˆ ì•ˆ ë“¤ê³  ì™”ë„¤\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ì˜¤ ì‹ ë°œ ìƒ€ëƒ\\nì‘  ì „ì— ì‹ ë˜ ê²Œ ë‹¤ í—¤ì ¸ì„œ  ë¶€ëª¨ë‹˜ì´ ì‚¬ì£¼ì…¨ì–´\\në„ˆë„¤ ì§‘ ê½¤ ì‚¬...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ì˜í•˜ í•™ìƒ ì´ë¦¬ ë‚˜ì™€ìš”\\në„¤ ì„ ìƒë‹˜\\nì„ ìƒë‹˜ì´ ì™œ ë¶€ë¥¸ ì¤„ ì•Œì•„\\nì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤ ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”</td>\n",
       "      <td>ì”¨ ì˜¤ëŠ˜ ë­ í•´ìš”\\nì•„ ì € ì•½ì†ì´ ìˆì–´ì„œìš”\\nëˆ„êµ¬ë‘ìš”  ë‚¨ìì¹œêµ¬\\nì•„  ë„¤\\nì—ì´ ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ê°ˆì·¨ ëŒ€í™”</td>\n",
       "      <td>ì•¼ ì£½ê¸° ì‹«ìœ¼ë©´ ëˆ ë‚´ë†”\\nì£„ì†¡í•©ë‹ˆë‹¤  ì—¬ê¸°\\në­” ì´ê±° ë°– ì—†ì–´  ë’¤ì ¸ì„œ ë‚˜ì˜¤ë©´ ì£½...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         class                                 non_label_sentence  label\n",
       "0  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  ê¸¸ë™ ì”¨ ì´ë²ˆì— ì´ê²ƒ ì¢€ ì²˜ë¦¬í•´ìš”\\nì´ê±° ì œê°€ í•œ ê²Œ ì•„ë‹Œë°ìš”\\níŒ€ì—ì„œ ë‚´ê°€ ë„¤ê°€ ...      2\n",
       "1    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì•¼ ë„ˆ ì•ë‹ˆ ì™œ ê·¸ë ‡ê²Œ íŠ€ì–´ë‚˜ì™”ëƒ\\níƒœì–´ë‚  ë•Œë¶€í„° ê·¸ë¬ì–´ ë¬¼ì–´ë³´ì§€ ë§ˆ\\nì•„ ê·¸ëŸ¼ íƒœ...      3\n",
       "2        ê°ˆì·¨ ëŒ€í™”  ì›í›„ì•¼ í•™ì› êµì¬ ì‚¬ì•¼ ë˜ëŠ”ë° 8ë§Œ ì›ë§Œ ì£¼ë©´ ì•ˆ ë¼\\në¯¸ì•ˆ ë‚˜ ì˜¤ëŠ˜ì€ ì§„ì§œ ëˆì´ ...      1\n",
       "3        ê°ˆì·¨ ëŒ€í™”  ë„ˆ ì €ë²ˆì— ìˆ  ë¨¹ì€ ë‚  ê¸°ì–µí•´\\nì•„ë‹ˆ ì™œ\\nì•¼ 300ë§Œ ì›ë§Œ ê°€ì ¸ì™€\\në­” ì†Œë¦¬ì•¼  ...      1\n",
       "4        í˜‘ë°• ëŒ€í™”  ë„ˆ ë˜ ë‚´ë§ ë¬´ì‹œí•˜ëƒ\\nì´ ìƒˆë¼ ë„ ì¢†ìœ¼ë¡œ ë³´ë‚˜ ë³¸ë°\\nì•„ëƒ ì§„ì§œ ì‹œê°„ì´ ì—†ì—ˆì–´ ë¯¸...      0\n",
       "5        ê°ˆì·¨ ëŒ€í™”  ì•¼ ë¼ì§€ ì˜¤ëŠ˜ë„ ëˆ ë‚´ë†”\\nì˜¤ëŠ˜ì€ ì§„ì§œ ì—†ì–´\\nì•„ìƒˆë¼ ì˜¤ëŠ˜ ì§„ì§œ ëˆ ì•ˆ ë“¤ê³  ì™”ë„¤\\...      1\n",
       "6    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì˜¤ ì‹ ë°œ ìƒ€ëƒ\\nì‘  ì „ì— ì‹ ë˜ ê²Œ ë‹¤ í—¤ì ¸ì„œ  ë¶€ëª¨ë‹˜ì´ ì‚¬ì£¼ì…¨ì–´\\në„ˆë„¤ ì§‘ ê½¤ ì‚¬...      3\n",
       "7    ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”  ì˜í•˜ í•™ìƒ ì´ë¦¬ ë‚˜ì™€ìš”\\në„¤ ì„ ìƒë‹˜\\nì„ ìƒë‹˜ì´ ì™œ ë¶€ë¥¸ ì¤„ ì•Œì•„\\nì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤ ...      3\n",
       "8  ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”  ì”¨ ì˜¤ëŠ˜ ë­ í•´ìš”\\nì•„ ì € ì•½ì†ì´ ìˆì–´ì„œìš”\\nëˆ„êµ¬ë‘ìš”  ë‚¨ìì¹œêµ¬\\nì•„  ë„¤\\nì—ì´ ...      2\n",
       "9        ê°ˆì·¨ ëŒ€í™”  ì•¼ ì£½ê¸° ì‹«ìœ¼ë©´ ëˆ ë‚´ë†”\\nì£„ì†¡í•©ë‹ˆë‹¤  ì—¬ê¸°\\në­” ì´ê±° ë°– ì—†ì–´  ë’¤ì ¸ì„œ ë‚˜ì˜¤ë©´ ì£½...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = {'í˜‘ë°• ëŒ€í™”':0, 'ê°ˆì·¨ ëŒ€í™”':1, 'ì§ì¥ ë‚´ ê´´ë¡­í˜ ëŒ€í™”':2, 'ê¸°íƒ€ ê´´ë¡­í˜ ëŒ€í™”':3, 'ì¼ë°˜ ëŒ€í™”':4}\n",
    "num_labels = len(labels)\n",
    "data['label'] = data['class'].apply(lambda x : labels[x])\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a714c5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from transformers import ElectraForSequenceClassification, RobertaForSequenceClassification\n",
    "from bert.modeling_bert import BertForSequenceClassification \n",
    "\n",
    "HUGGINGFACE_MODEL_PATH = \"klue/bert-base\"\n",
    "# \"klue/roberta-large\"\n",
    "# \"beomi/kcbert-large\"\n",
    "# \"beomi/KcELECTRA-base-v2022\"\n",
    "# \"klue/bert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(HUGGINGFACE_MODEL_PATH)\n",
    "#model = AutoModel.from_pretrained(\"beomi/KcELECTRA-base\")\n",
    "model = BertForSequenceClassification.from_pretrained(HUGGINGFACE_MODEL_PATH, num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90c9155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(pa.Table.from_pandas(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a58e6df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869aaffc4d9949f08ae3dc7c613e821c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform(data):\n",
    "  return tokenizer(\n",
    "      data['non_label_sentence'],\n",
    "      truncation = True,\n",
    "      padding = 'max_length',\n",
    "      return_token_type_ids = True,\n",
    "      )\n",
    "\n",
    "encoded_data = data.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3de8316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset= encoded_data.train_test_split(test_size=0.1).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1213ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['class', 'non_label_sentence', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 24277\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10b48d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wlsaud5474/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels, average=\"micro\")\n",
    "\n",
    "output_dir = os.getenv('HOME')+'/aiffelthon/AIFFELTHON/Fine_tuning/KcBERT-large'\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir, \n",
    "    evaluation_strategy=\"epoch\", #evaluationí•˜ëŠ” ë¹ˆë„\n",
    "    learning_rate = 2e-5, \n",
    "    per_device_train_batch_size = 8, \n",
    "    per_device_eval_batch_size = 8, \n",
    "    num_train_epochs = 2,\n",
    "    weight_decay = 0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f5a8de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: class, non_label_sentence. If class, non_label_sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/wlsaud5474/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 24277\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6070\n",
      "  Number of trainable parameters = 110621189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34' max='6070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  34/6070 00:08 < 25:26, 3.95 it/s, Epoch 0.01/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3711/1814470528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
      "\u001b[0;32m~/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2526\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[1;32m    487\u001b[0m         torch.autograd.backward(\n\u001b[0;32m--> 488\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         )\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/jinmyeong/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                           \n",
    "    args=training_arguments,                  \n",
    "    train_dataset=train_dataset,    \n",
    "    eval_dataset=validation_dataset,      \n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d901c4",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = os.getenv('HOME')+'/aiffelthon/AIFFELTHON/TUNiB/test.json'\n",
    "with open(test_file_path, mode='rt', encoding='utf-8') as f:\n",
    "    test_dataset = pd.read_json(f)\n",
    "\n",
    "test_data = test_dataset.transpose()\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d6b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_punctuation(x):\n",
    "  x = x.strip()\n",
    "  x = re.sub(\"[^ê°€-í£0-9]+\", \" \", x)\n",
    "  x = re.sub(\"[ ]+\", \" \", x)\n",
    "  x = x.strip()\n",
    "  return x\n",
    "\n",
    "test_data[\"text\"] = test_data['text'].apply(lambda x : remove_punctuation(x))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Dataset(pa.Table.from_pandas(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5607d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_test(data):\n",
    "  return tokenizer(\n",
    "      data['text'],\n",
    "      truncation = True,\n",
    "      padding = 'max_length',\n",
    "      return_token_type_ids = True,\n",
    "      )\n",
    "\n",
    "encoded_test = test.map(transform_test, batched=True)\n",
    "encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6323bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer.predict(encoded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853ec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = []\n",
    "\n",
    "for i in range(len(pred[0])):\n",
    "    pred_label.append((str)(np.argmax(pred[0][i])).zfill(2))\n",
    "    \n",
    "pred_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fceb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_data.assign(CLASS=pred_label)\n",
    "submission = submission.rename(columns={'CLASS':'class'})\n",
    "\n",
    "submission.drop(['text'], axis=1, inplace=True)\n",
    "submission = submission.transpose()\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# sub_dir = os.getenv('HOME')+'/aiffelthon/AIFFELTHON/TUNiB/submission'\n",
    "# submission_file_path = f'{sub_dir}/submission_klue-bert-base-en_fr_ch_8000.json'\n",
    "# result = submission.to_json(submission_file_path)\n",
    "\n",
    "# with open(submission_file_path) as f:\n",
    "#     parsed = json.load(f)\n",
    "\n",
    "# with open(submission_file_path, 'w') as f:\n",
    "#     json.dump(parsed, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
