{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUJdwb8YGCG3",
    "outputId": "dff4ebc5-70e2-4d3d-b43a-6f18745c0ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soynlp in /opt/conda/lib/python3.9/site-packages (0.0.493)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.9/site-packages (from soynlp) (1.0)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /opt/conda/lib/python3.9/site-packages (from soynlp) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from soynlp) (1.7.1)\n",
      "Requirement already satisfied: psutil>=5.0.1 in /opt/conda/lib/python3.9/site-packages (from soynlp) (5.8.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->soynlp) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->soynlp) (3.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ppAVcCev7Td6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from konlpy.tag import Okt, Mecab\n",
    "from soynlp.word import WordExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8cyv2XpOllF"
   },
   "source": [
    "# 데이터\n",
    "- 전처리 및 맞춤법 검사가 된 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "CJ3lGtgaCMCz",
    "outputId": "3659dc8b-dc24-4f8e-8f4f-d7035fc5833a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>non_label_sentence</th>\n",
       "      <th>label_sentence</th>\n",
       "      <th>class</th>\n",
       "      <th>binary_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>해병대는 자기가 지원해서 가는 거잖아\\n응응 시험 같은 것도 있지 않아\\n어떻게 해...</td>\n",
       "      <td>1:해병대는 자기가 지원해서 가는 거잖아\\n2:응응 시험 같은 것도 있지 않아\\n1...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>오늘 서울은 하루종일 꾸물하다 날씨가\\n요즘 날씨가 계속 꾸물하고 비오고\\n언제 선...</td>\n",
       "      <td>1:오늘 서울은 하루종일 꾸물하다 날씨가\\n2:요즘 날씨가 계속 꾸물하고 비오고\\n...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>남자들은 전립선 비대증이 큰일이에요\\n진작에 관리 안 한 걸 후회한다\\n치료도 잘 ...</td>\n",
       "      <td>1:남자들은 전립선 비대증이 큰일이에요\\n2:진작에 관리 안 한 걸 후회한다\\n1:...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>안녕\\n나는 이 날씨에 서핑하러 간다\\n와 이제 곧 겨울인데 서핑을 해\\n당연히 제...</td>\n",
       "      <td>1:안녕\\n1:나는 이 날씨에 서핑하러 간다\\n2:와 이제 곧 겨울인데 서핑을 해\\...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>넌 몇살에 결혼 하고 싶어\\n35살 난 최대한 즐기고 결혼 할 거야\\n그럼 애기는 ...</td>\n",
       "      <td>1:넌 몇살에 결혼 하고 싶어\\n2:35살 난 최대한 즐기고 결혼 할 거야\\n1:그...</td>\n",
       "      <td>일반 대화</td>\n",
       "      <td>일반 대화</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  non_label_sentence  \\\n",
       "0  해병대는 자기가 지원해서 가는 거잖아\\n응응 시험 같은 것도 있지 않아\\n어떻게 해...   \n",
       "1  오늘 서울은 하루종일 꾸물하다 날씨가\\n요즘 날씨가 계속 꾸물하고 비오고\\n언제 선...   \n",
       "2  남자들은 전립선 비대증이 큰일이에요\\n진작에 관리 안 한 걸 후회한다\\n치료도 잘 ...   \n",
       "3  안녕\\n나는 이 날씨에 서핑하러 간다\\n와 이제 곧 겨울인데 서핑을 해\\n당연히 제...   \n",
       "4  넌 몇살에 결혼 하고 싶어\\n35살 난 최대한 즐기고 결혼 할 거야\\n그럼 애기는 ...   \n",
       "\n",
       "                                      label_sentence  class binary_class  \n",
       "0  1:해병대는 자기가 지원해서 가는 거잖아\\n2:응응 시험 같은 것도 있지 않아\\n1...  일반 대화        일반 대화  \n",
       "1  1:오늘 서울은 하루종일 꾸물하다 날씨가\\n2:요즘 날씨가 계속 꾸물하고 비오고\\n...  일반 대화        일반 대화  \n",
       "2  1:남자들은 전립선 비대증이 큰일이에요\\n2:진작에 관리 안 한 걸 후회한다\\n1:...  일반 대화        일반 대화  \n",
       "3  1:안녕\\n1:나는 이 날씨에 서핑하러 간다\\n2:와 이제 곧 겨울인데 서핑을 해\\...  일반 대화        일반 대화  \n",
       "4  1:넌 몇살에 결혼 하고 싶어\\n2:35살 난 최대한 즐기고 결혼 할 거야\\n1:그...  일반 대화        일반 대화  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/aiffel/train_10000_mk2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 3.252 Gbory 3.144 Gb\n",
      "all cohesion probabilities was computed. # words = 2803\n",
      "all branching entropies was computed # words = 58000\n",
      "all accessor variety was computed # words = 58000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13870, 195)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = data.non_label_sentence\n",
    "sentences = [sen for sen in sentences]\n",
    "\n",
    "from soynlp.tokenizer import LTokenizer\n",
    "vocab_size = 30000\n",
    "\n",
    "word_extractor = WordExtractor(\n",
    "    min_frequency=100, # example\n",
    "    min_cohesion_forward=0.05,\n",
    "    min_right_branching_entropy=0.0\n",
    ")\n",
    "\n",
    "word_extractor.train(sentences)\n",
    "words = word_extractor.extract()\n",
    "\n",
    "cohesion_score = {word:score.cohesion_forward for word, score in words.items()}\n",
    "tokenizer = LTokenizer(scores=cohesion_score)\n",
    "\n",
    "sentences = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "tokenizer_tf = Tokenizer(num_words=vocab_size)\n",
    "tokenizer_tf.fit_on_texts(sentences)\n",
    "word_dic = tokenizer_tf.word_index\n",
    "sequences = tokenizer_tf.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences)\n",
    "np.shape(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126458"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecw2j-NTIvvu",
    "outputId": "6dd99533-cdd5-450b-fa09-be78c32752e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13870 13870\n"
     ]
    }
   ],
   "source": [
    "train_data = padded\n",
    "train_label = data['class']\n",
    "print(len(train_data), len(train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qt0u-2UTKUsF",
    "outputId": "3789632e-f39d-4718-e403-a144f0db6713"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11096 1387 1387\n",
      "11096 1387 1387\n"
     ]
    }
   ],
   "source": [
    "labels = {'직장 내 괴롭힘 대화': 2, '기타 괴롭힘 대화': 3, '갈취 대화': 1, '협박 대화': 0, '일반 대화': 4}\n",
    "train_label = train_label.apply(lambda x: labels[x])\n",
    "train_label = pd.get_dummies(train_label)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_data, train_label, test_size=0.2, random_state=22)\n",
    "valid_X, test_X, valid_Y, test_Y = train_test_split(test_X, test_Y, test_size=0.5, random_state=22)\n",
    "\n",
    "print(len(train_X), len(valid_X), len(test_X))\n",
    "print(len(train_Y), len(valid_Y), len(test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzB2haBJLJoH"
   },
   "source": [
    "# 모델 \n",
    "- LSTM 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8kkPNDrLKZ7",
    "outputId": "8457f8f7-ebd4-4ea3-b938-93b4c4e8b1e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 1024)        30720000  \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, None, 128)         590336    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 30000)             1950000   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 30000)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 5)                 150005    \n",
      "=================================================================\n",
      "Total params: 33,591,333\n",
      "Trainable params: 33,591,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_vector_dim = 1024\n",
    "labels_size = len(labels)\n",
    "hidden_size = 128\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size//2))\n",
    "model.add(tf.keras.layers.Dense(vocab_size, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.4))\n",
    "model.add(tf.keras.layers.Dense(labels_size, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "a0sS-wccLaok",
    "outputId": "78166618-b960-4af9-b3da-dabbb73cdf2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "44/44 [==============================] - 9s 137ms/step - loss: 0.6831 - accuracy: 0.7441 - val_loss: 0.4342 - val_accuracy: 0.7823\n",
      "Epoch 2/10\n",
      "44/44 [==============================] - 5s 118ms/step - loss: 0.3896 - accuracy: 0.8158 - val_loss: 0.3502 - val_accuracy: 0.8392\n",
      "Epoch 3/10\n",
      "44/44 [==============================] - 5s 119ms/step - loss: 0.2226 - accuracy: 0.9058 - val_loss: 0.2357 - val_accuracy: 0.9084\n",
      "Epoch 4/10\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.0811 - accuracy: 0.9728 - val_loss: 0.2287 - val_accuracy: 0.9279\n",
      "Epoch 5/10\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 0.0254 - accuracy: 0.9918 - val_loss: 0.2877 - val_accuracy: 0.9265\n",
      "Epoch 6/10\n",
      "44/44 [==============================] - 5s 120ms/step - loss: 0.0133 - accuracy: 0.9961 - val_loss: 0.3201 - val_accuracy: 0.9351\n",
      "Epoch 7/10\n",
      "44/44 [==============================] - 5s 121ms/step - loss: 0.0091 - accuracy: 0.9971 - val_loss: 0.3309 - val_accuracy: 0.9358\n",
      "Epoch 8/10\n",
      "44/44 [==============================] - 5s 122ms/step - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.3730 - val_accuracy: 0.9236\n",
      "Epoch 9/10\n",
      "29/44 [==================>...........] - ETA: 1s - loss: 0.0058 - accuracy: 0.9981"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs=10\n",
    "\n",
    "history = model.fit(train_X,\n",
    "                    train_Y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(valid_X, valid_Y),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_file_path = '/aiffel/test.json'\n",
    "with open(test_file_path, mode='rt', encoding='utf-8') as f:\n",
    "    test_dataset = pd.read_json(f)\n",
    "    \n",
    "test_data = test_dataset.transpose()\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentences = [sen for sen in test_data['text']]\n",
    "sentences = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "sequences = tokenizer_tf.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences)\n",
    "np.shape(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(padded)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pred_label = []\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    pred_label.append((str)(np.argmax(pred[i])))\n",
    "\n",
    "pred_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_change = {'0':'00', '1':'01', '2':'02', '3':'03', '4':'04'}\n",
    "\n",
    "sub_label = []\n",
    "\n",
    "for pre in pred_label:\n",
    "    sub_label.append(label_change[pre])\n",
    "\n",
    "sub_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_data.assign(CLASS=sub_label)\n",
    "submission = submission.rename(columns={'CLASS':'class'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "submission.drop(['text'], axis=1, inplace=True)\n",
    "submission = submission.transpose()\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "submission_file_path = '/aiffel/submission_LSTM_10000'\n",
    "result = submission.to_json(submission_file_path)\n",
    "\n",
    "with open(submission_file_path) as f:\n",
    "    parsed = json.load(f)\n",
    "\n",
    "with open(submission_file_path, 'w') as f:\n",
    "    json.dump(parsed, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
